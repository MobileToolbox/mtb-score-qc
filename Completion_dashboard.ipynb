{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-animal",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import synapseclient\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import ipywidgets as widgets\n",
    "# %matplotlib widget #breaks javascript in notebook\n",
    "\n",
    "import create_summary\n",
    "\n",
    "COMPLETION_COLS = ['wasCompleted', 'inSynapseParent', 'inStudyProject', 'inParquet', 'inScores']\n",
    "DELTA_T_COLS = ['completion time [s]', 'export delay [s]', 'upload delay [s]', 'in Synapse delay [s]']\n",
    "\n",
    "syn = synapseclient.login();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-reliance",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load Data \n",
    "studies = create_summary.getStudies(syn)\n",
    "\n",
    "def readCompletionFile(synId, version=None):\n",
    "    df = pd.read_csv(syn.get(synId, version=version).path, low_memory=False)\n",
    "    timeCols =  ['uploadedOn', 'finishedOn', 'startedOn', 'eventTimestamp_x', 'modifiedOn', 'createdOn', 'exportedOn', 'eventTimestamp_y']\n",
    "    df[timeCols] = df[timeCols].apply(pd.to_datetime)\n",
    "\n",
    "    #Fill in startedOn data for columns and remove fnamea\n",
    "    df = (df.assign(startedOn = np.where(df.startedOn.isnull(), df.uploadedOn, df.startedOn))\n",
    "          .query(\"assessmentId!='fnamea'\")\n",
    "          .set_index('startedOn', drop=True))\n",
    "    return df\n",
    "\n",
    "df = readCompletionFile('syn51185576') \n",
    "df['completion time [s]'] = (df.finishedOn-df.index).map(lambda x:x.seconds)\n",
    "df['export delay [s]'] = (df.exportedOn-df.finishedOn).map(lambda x:x.seconds)\n",
    "df['upload delay [s]'] = (df.uploadedOn-df.finishedOn).map(lambda x:x.seconds)\n",
    "df['in Synapse delay [s]'] = (df.createdOn-df.finishedOn).map(lambda x:x.seconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-cisco",
   "metadata": {},
   "source": [
    "# Percent complete Data\n",
    "Percent of data that was reported as completed by adherence tracking in Bridge that has made it to different stages in the pipeline.$^{1}$ Number of records at each stage is reported by n.\n",
    "\n",
    "$^{1.}$<font size=\"1\">Percents can be higher than 100 if the data was uploaded but the adherence records was not uploaded to Bridge.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-thesaurus",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp_df = df.dropna(subset=['wasCompleted'])[COMPLETION_COLS]\n",
    "n_alltime = temp_df.sum()\n",
    "pct_alltime = pd.DataFrame(n_alltime.div(n_alltime['wasCompleted'], axis=0)*100, columns=['All Time'])\n",
    "labels_alltime = pd.DataFrame([f\"{a:.0f}%\\n\\nn={b:,.0f}\" for a, b in zip(pct_alltime['All Time'], n_alltime)])\n",
    "\n",
    "plt.figure(figsize=(13,2.5))\n",
    "#ax = sns.heatmap(pct_alltime.T, annot=True, fmt=\".0f\", cbar=False, linewidth=5, vmin=0);\n",
    "ax = sns.heatmap(pct_alltime.T, annot=labels_alltime.T, fmt=\"s\", cbar=False, linewidth=5, vmin=0,\n",
    "                annot_kws={\"fontsize\":22});\n",
    "plt.yticks([])\n",
    "plt.title('All Time', fontsize=22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-organ",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_lastweek = temp_df.groupby(pd.Grouper(freq='W')).sum().iloc[-2]\n",
    "legend = n_lastweek.name.strftime('week ending %d-%b-%Y')\n",
    "n_lastweek.name = legend\n",
    "pct_lastweek  = pd.DataFrame(n_lastweek.div(n_lastweek['wasCompleted'], axis=0)*100)\n",
    "labels_lastweek = pd.DataFrame([f\"{a:.0f}%\\n\\nn={b:,.0f}\" for a, b in zip(pct_lastweek[legend], n_lastweek)])\n",
    "\n",
    "#n_lastweek\n",
    "plt.figure(figsize=(13,2.5))\n",
    "#ax = sns.heatmap(pct_lastweek.T, annot=True, fmt=\".0f\", cbar=False, linewidth=5,vmin=0);\n",
    "ax = sns.heatmap(pct_lastweek.T, annot=labels_lastweek.T, fmt=\"s\", cbar=False, linewidth=5, vmin=0,\n",
    "                annot_kws={\"fontsize\":22});\n",
    "plt.yticks([])\n",
    "plt.title(legend, fontsize=22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-supervisor",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "n_lastweek = temp_df.groupby(pd.Grouper(freq='W')).sum().iloc[-1]\n",
    "legend = n_lastweek.name.strftime('week ending %d-%b-%Y')\n",
    "n_lastweek.name = legend\n",
    "pct_lastweek  = pd.DataFrame(n_lastweek.div(n_lastweek['wasCompleted'], axis=0)*100)\n",
    "labels_lastweek = pd.DataFrame([f\"{a:.0f}%\\n\\nn={b:,.0f}\" for a, b in zip(pct_lastweek[legend], n_lastweek)])\n",
    "\n",
    "#n_lastweek\n",
    "plt.figure(figsize=(13,2.5))\n",
    "#ax = sns.heatmap(pct_lastweek.T, annot=True, fmt=\".0f\", cbar=False, linewidth=5,vmin=0);\n",
    "ax = sns.heatmap(pct_lastweek.T, annot=labels_lastweek.T, fmt=\"s\", cbar=False, linewidth=5, vmin=0,\n",
    "                annot_kws={\"fontsize\":22});\n",
    "plt.yticks([])\n",
    "plt.title(legend, fontsize=22);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-works",
   "metadata": {},
   "source": [
    "### Last Scored data by Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-engineer",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "(studies[['studyId', 'name']]\n",
    " .merge(df.query(\"inScores==1\").groupby('studyId')['finishedOn'].max(), on='studyId')\n",
    ").set_index('studyId').sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-uncertainty",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "ax2 = plt.subplot2grid((4, 1), (0, 0), rowspan=1, colspan=1)\n",
    "(df.query(\"wasCompleted==1\").groupby(pd.Grouper(freq='W'))['wasCompleted'].count()).plot(ax=ax2, kind='bar')\n",
    "plt.ylabel('Nr Records');\n",
    "plt.title('Data Status Across Time')\n",
    "plt.xticks([]);\n",
    "plt.xlabel('')\n",
    "ax1 = plt.subplot2grid((4, 1), (1, 0), rowspan=3, colspan=1)\n",
    "(df.query(\"wasCompleted==1\").groupby(pd.Grouper(freq='W'))[COMPLETION_COLS].mean()*100).plot(ax=ax1)\n",
    "plt.ylim(0,102)\n",
    "plt.ylabel('completed [%]');\n",
    "ticks, labels = plt.xticks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-memorabilia",
   "metadata": {},
   "source": [
    "### Changes in processing across time\n",
    "\n",
    "The following graphs looks at the state of the data as it appeared at different times since March-2023.  That is, as improvements have been made and data recovered the counts have changed.  This uses historical snapshots stored in versions of [syn51185576](https://www.synapse.org/#!Synapse:syn51185576) to infer counts of records and percentages.  This is currently done for data collected across all time and data collected since January-2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-worship",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def summarizeCompletion(df):\n",
    "    \"\"\"Summarized state of completion for a specific dataframe.\n",
    "    \n",
    "    Args\n",
    "        df - completion data frame\n",
    "        \n",
    "    returns - data frame with date column label and rows with summaries.\n",
    "    \"\"\"\n",
    "    #Only look at records where we have a completion record\n",
    "    df = df.dropna(subset=['wasCompleted'])[COMPLETION_COLS]\n",
    "\n",
    "    date = df.index.max()\n",
    "    n_alltime = df.sum()\n",
    "    pct_alltime = pd.DataFrame(n_alltime.div(n_alltime['wasCompleted'], axis=0)*100, columns=['All Time'])\n",
    "    #Summarize since jan-1-2023\n",
    "    df = df[df.index>'2023']\n",
    "    n_2023 = df.sum()\n",
    "    pct_2023 = pd.DataFrame(n_2023.div(n_2023['wasCompleted'], axis=0)*100, columns=['All Time'])\n",
    "    return pd.concat([n_alltime, pct_alltime['All Time'], n_2023, pct_2023['All Time']], \n",
    "               keys=[('beginning', 'N'), ('beginning','pct'), ('2023', 'N'), ('2023','pct')], \n",
    "               names=['timeFrame','summary']).to_frame(name=date)\n",
    "    \n",
    "    \n",
    "maxVersion = syn.get('syn51185576', downloadFile=False).versionNumber\n",
    "dfs = [readCompletionFile('syn51185576', version) \n",
    "       for version in range(7, maxVersion+1)]\n",
    "summary = pd.concat([summarizeCompletion(df) for df in dfs], axis=1).T\n",
    "\n",
    "plt.figure(figsize=(13,4))\n",
    "for i, t in enumerate(['beginning', '2023']):\n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "    (summary[[(t, 'pct','inParquet'), (t, 'pct','inScores'),(t, 'pct','inStudyProject')]]\n",
    "        .droplevel([0,1], axis=1)\n",
    "        .plot(ax=ax))\n",
    "    plt.ylabel('Percent available')\n",
    "    plt.xlabel('Date scored')\n",
    "    plt.title(f'Data generated since {t}')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-principle",
   "metadata": {},
   "source": [
    "# Associations between missing data and conditions when collected\n",
    "Evaluates whether there are any specific conditions that lead to a higher risk of missing data. For example whether the appVersion or the sessioon is assoicated with data being missing in the **scores** stage. Can also be modified to identify other missingness for example in upload or parquet data. **Only looks at data from 'completed' assessments, i.e those that weren't declined.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-repair",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Columns that are evaluated\n",
    "correlated_columns=['wasCompleted','assessmentId', 'studyId', 'appVersion','assessmentRevision', 'clientTimeZone', 'declined', \n",
    "                    'deviceName', 'languages','osName', 'osVersion', 'scheduleGuid', 'sessionGuid', \n",
    "                    'sessionInstanceGuid', 'sessionInstanceStartDay', 'sessionStartEventId']\n",
    "\n",
    "@widgets.interact(stage_missing=['inScores', 'inParquet', 'inStudyProject'], \n",
    "                  end_date = df.index.max().strftime('%Y-%m-%d %X'),\n",
    "                  p_cut_off=widgets.fixed(1e-3))\n",
    "def find_correlated_variables(start_date='2023-03-01 00:00:00', end_date='2023-04-01 00:00:00', stage_missing='inScores', p_cut_off=0.001):\n",
    "    \"\"\"Filters the data by timeframe and compares the differences in missingness in a specific stage in\n",
    "       pipeline.  Computes contigency tables and determines if differences are statistically significant and plot\n",
    "       the odds of missing data.\n",
    "       \n",
    "    \"\"\"\n",
    "    temp_df = df[df.index > start_date].query('wasCompleted==1')\n",
    "    temp_df = temp_df[temp_df.index<end_date]\n",
    "    nMissingSynapse = sum(temp_df['inStudyProject']==0)\n",
    "    nMissingParquet = sum(temp_df['inParquet']==0)\n",
    "    nMissingScores = sum(temp_df['inScores']==0)\n",
    "    print(f'Between {start_date} and {end_date}: {nMissingScores} ({100*nMissingScores/len(temp_df):0.2f}%) missing out of {len(temp_df)}')\n",
    "    print(f'{nMissingSynapse} ({100*nMissingSynapse/len(temp_df):0.2f}%) records missing in Synapse')\n",
    "    print(f'{nMissingParquet-nMissingSynapse} ({100*(nMissingParquet-nMissingSynapse)/len(temp_df):0.2f}%) additional missing in Parquet')\n",
    "    print(f'{nMissingScores-nMissingParquet} ({100*(nMissingScores-nMissingParquet)/len(temp_df):0.2f}%) additional missing from Scores')\n",
    "    \n",
    "    #Filter out the previous stage so we only focus on data missing between the previous stage and the \"stage_missing\"\n",
    "    if stage_missing == 'inParquet':\n",
    "        temp_df = temp_df.query(\"inStudyProject==1\")\n",
    "    elif stage_missing == 'inScores':\n",
    "        temp_df = temp_df.query(\"inParquet==1\")\n",
    "\n",
    "    for variable in correlated_columns:\n",
    "        #Create contigency table\n",
    "        contigency = (temp_df[[stage_missing, variable]]\n",
    "             .groupby(variable)[stage_missing]\n",
    "             .value_counts()  #Count number of missing and nn missing\n",
    "             .to_frame(name='available')      \n",
    "             .reset_index(1)\n",
    "             .pivot_table(columns=stage_missing, values='available', index=variable)\n",
    "             .fillna(0)  #If there are no counts set the NaN to 0\n",
    "             .rename(columns={1.0:'Present', 0.0:'Missing'})\n",
    "        )\n",
    "        # Filtering out any categories with too few measurements \n",
    "        contigency = contigency[contigency.sum(axis=1)>10]\n",
    "        # Only continue of data is missing\n",
    "        if 'Missing' not in contigency.columns:\n",
    "            continue\n",
    "        # Compute Chi-square statistic for missingness being different between groups\n",
    "        # Assumption there is no relationship between variable and missingness\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(contigency)\n",
    "    \n",
    "        # odds of missing data = (# 0s)/(# 1s)\n",
    "        contigency['Odds']=contigency['Missing']/contigency['Present']\n",
    "    \n",
    "        #Plot odds of missingness for each category if the p-value<0.001\n",
    "        if p<p_cut_off:\n",
    "            plt.figure(figsize=(13,6))\n",
    "            ax=plt.subplot(2,1,1)\n",
    "            contigency.plot(y='Odds', kind='bar', ax=ax);\n",
    "            plt.ylabel('Odds of Missing')\n",
    "            plt.title('%s p=%0.2g' %(variable, p))\n",
    "            ax=plt.subplot(2,1,2)\n",
    "            #Format table for display\n",
    "            contigency['Odds'] = contigency['Odds'].apply('{:,.1f}'.format)\n",
    "            contigency = contigency.astype({'Present': int, 'Missing': int})\n",
    "            plt.table(cellText=contigency.values.T, rowLabels=contigency.columns, loc='center')\n",
    "            plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-monkey",
   "metadata": {},
   "source": [
    "# Study Specific information\n",
    "Filter down to a specific study and explore the data within that study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-knitting",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "groups = df.groupby('studyId')\n",
    "@widgets.interact(study=groups.groups.keys())\n",
    "def plot_studies(study):\n",
    "    plt.figure(figsize=(2,8))\n",
    "    temp_df = groups.get_group(study)[COMPLETION_COLS]\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    sns.heatmap(temp_df, cbar=False, ax=ax)\n",
    "    ticklabels = ['NaT' if pd.isnull(temp_df.index[int(tick)]) else temp_df.index[int(tick)].strftime('%Y-%m-%d') for tick in ax.get_yticks()]\n",
    "    ax.set_yticklabels(ticklabels);\n",
    "    label = (studies.query(\"studyId=='%s'\"%study)['name']+'('+ studies.query(\"studyId=='%s'\"%study)['id'] + ')').iloc[0]\n",
    "    plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-ghost",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@widgets.interact(study=groups.groups.keys(), stages=COMPLETION_COLS )\n",
    "def show_participant_level_data(study='hktrrx', stages='inStudyProject'):\n",
    "    return (df.query(\"studyId=='%s'\" %study)\n",
    "           .groupby(['externalId', 'sessionGuid'])[stages]\n",
    "           .apply(sum).reset_index()\n",
    "           .pivot(index='externalId', columns='sessionGuid')\n",
    "           .fillna(0)\n",
    "           .astype(int))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-album",
   "metadata": {},
   "source": [
    "# Timing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-aerospace",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "    \n",
    "timing = df.groupby(pd.Grouper(freq='W'))[DELTA_T_COLS].median()\n",
    "sns.lineplot(data=timing)\n",
    "plt.ylabel('Delay [s]');\n",
    "plt.yscale('log')\n",
    "timing = timing.reset_index().melt(id_vars=['startedOn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-factory",
   "metadata": {},
   "source": [
    "# Apps in active use\n",
    "App Versions being used across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-today",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "oses = ['Android', 'iPadOS', 'iPhone OS']\n",
    "# Combine and count appVersions by week\n",
    "appCounts = (df.dropna(subset=['wasCompleted'])\n",
    "             .groupby([pd.Grouper(freq='W'),'osName', 'appVersion'])[['healthCode']]\n",
    "             .nunique()\n",
    "             .reset_index())\n",
    "appCounts = appCounts.assign(penetration = appCounts.healthCode/appCounts.groupby(['startedOn', 'osName']).healthCode.transform('sum'))\n",
    "appCounts = appCounts.pivot_table(index='startedOn', columns=['osName', 'appVersion'], values='penetration').fillna(0)\n",
    "\n",
    "# Plot and outoput last week \n",
    "plt.figure(figsize=(13,13))\n",
    "for i, os in enumerate(oses):\n",
    "    ax=plt.subplot(3,1,i+1)\n",
    "    appCounts.loc[:, os].plot(ax=ax)\n",
    "    plt.title(os)\n",
    "print('This weeks usage in percent by OS')\n",
    "#with pd.option_context('display.float_format', '{:,.2f}%'.format):\n",
    "last_week = appCounts.loc[:, ['Android', 'iPhone OS']].iloc[-1:]\n",
    "(last_week.T[last_week.T!=0].dropna()*100).round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-granny",
   "metadata": {},
   "source": [
    "## Number of Active Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-sullivan",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,5))\n",
    "userCount = df.groupby(pd.Grouper(freq='M'))['healthCode'].nunique()\n",
    "userCount.plot()\n",
    "plt.title('Nr Active users by month');\n",
    "print('Number of users in this month:', userCount[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-responsibility",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (global)",
   "language": "python",
   "name": "py3global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
